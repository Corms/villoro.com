# --------------------------------------------------------------------------------------------------
# Basic metadata
# --------------------------------------------------------------------------------------------------
code: google_analytics_etl
title: Google Analytics ETL
#title_short: 
date: "2019-05-26"
image: default.png
highlight: True

tags:
  - Python
  - Pandas
  - ETL
  - API
  - Google Analytics

tags_filter:
  - Python
  - Pandas
  - ETL

# --------------------------------------------------------------------------------------------------
# Extra info. This will add a button with href to the url
# --------------------------------------------------------------------------------------------------
# link: 
#   text: Github
#   url: 


# --------------------------------------------------------------------------------------------------
# Content
# --------------------------------------------------------------------------------------------------
brief_markdown: |
  brief_markdown

# image_head:
#   filename: numba.svg
#   caption: numba

content_markdown: |
  Google Analytics is really useful but sometimes you want retrive all the data and perform your own analysis and create your reports.
  In this post you will see how to do that with python.

  ## 1. Google permisions and keys

  ### 1.1. Get the Google key

  The first thing to do is to download the Google key. To do so you only need to follow
  [Google Analytics API](https://developers.google.com/analytics/devguides/reporting/core/v3/quickstart/service-py)
  steps.

  To sum them up you will need to:

  1. Go to [google service accounts page](https://console.developers.google.com/iam-admin/serviceaccounts).
  2. Make sure to select the desired project from the navigation bar (at the top).
  3. Start to create a new service account
  4. Give it some roles (optional)
  5. Create a `json` private key
  6. Download that key. It will allow you to access google data

  ### 1.2. Enable Google Analytics API

  You will need to enable the usage of the Google Analytics API with:

  1. Go to [API in Google Developers console](https://console.developers.google.com/apis/library/analytics.googleapis.com).
  2. Make sure to select the desired project from the navigation bar (at the top).
  3. Click enable

  ### 1.3. Allow the user to see Google Analytics data

  From the previous step you created and account with an email similar to `your_project@your_web.iam.gserviceaccount.com`
  (you obviously need to change `your_web` and `your_project`). Then give it read permisions with:

  1. Go to [Google Analytics web](https://analytics.google.com/).
  2. Select the **Admin** section from the left bar
  3. Click `Manage Users` at account level. **It is important that you give it at the account level** in order to work.
  4. Add the user with the email from the previous step
  5. Give it read access.

  > It can take some time until the new user has reading permisions.

  ### 1.4. Profile ID

  Get the ID that you will need to use with the API. There are three levels and the ID that you need to use is in the last one, the one with the red mark in the image.

  <div class="w3-center">
    <img src="/static/images/posts/google_analytics_profile.jpg" alt="google_analytics_profile" class="w3-image w3-padding-16" style="max-height: 600px;"/>
  </div>

  ## 2. Test the conection

  You can run the `test_conexion.py` code to check that everything is working.

  <div class="input">
    test_conexion.py
  </div>
  ```python
  from datetime import date, timedelta

  from apiclient.discovery import build
  from oauth2client.service_account import ServiceAccountCredentials


  # Those are the two constants you need to change
  FILE_GA_KEY = "" path tho the key file
  PROFILE = "" profile ID


  def get_ga_service():
      """ Connect to GA API service"""

      credentials = ServiceAccountCredentials.from_json_keyfile_name(
          FILE_GA_KEY, scopes=["https://www.googleapis.com/auth/analytics.readonly"]
      )

      # Build the service object.
      return build("analytics", "v3", credentials=credentials)


  def test():
      """ Tests the conexion """

      service = get_ga_service()

      # Feel free to change those values
      end = date.today()
      dimensions = ["ga:date", "ga:deviceCategory"]
      metrics = ["ga:users", "ga:sessions"]
      start = end - timedelta(7)

      # Query the API
      kwa = {
          "ids": "ga:{}".format(PROFILE),
          "start_date": "{:%Y-%m-%d}".format(start),
          "end_date": "{:%Y-%m-%d}".format(end),
          "metrics": ",".join(metrics),
          "dimensions": ",".join(dimensions),
          "max_results": 20,
      }

      return service.data().ga().get(**kwa).execute()

  ```

  > Remeber to edit both `FILE_GA_KEY` and `PROFILE` constants using the values from the previous steps.

  ## 3. Building the ETL
  ### 3.1. Decide metrics and dimensions

  Now is the time to decide what you want to download. There are two types of parameters **metrics** and **dimensions**.

  * **Dimensions** are atributes of your data
  * **Metrics** are quantitative measurements

  You can check with [query explorer](https://ga-dev-tools.appspot.com/query-explorer/) the possible values for both
  of them and you can also try your queries.

  ### 3.2. Extract and transform the data

  In this step I will descrive functions in the `etl.py` file.

  The first thing to do is to create the Google Analytics service with:

  <div class="input">
    etl.py
  </div>
  ```python
  def get_ga_service(key_file_location=None):
      """ Connect to GA API service"""

      if key_file_location is None:
          key_file_location = c.FILE_GA_KEY

      scope = "https://www.googleapis.com/auth/analytics.readonly"

      credentials = ServiceAccountCredentials.from_json_keyfile_name(
          key_file_location, scopes=[scope]
      )

      # Build the service object.
      return build("analytics", "v3", credentials=credentials)
  ```

  This function takes the default path of the Google Analytics key from the constants while allowing to pass a custom one.

  Then to retrive the actual data you should use `get_da_df` function.

  <div class="input">
    etl.py
  </div>
  ```python
  def get_ga_df(query_data, end=date.today(), service=None):
      """ Retrive GA data as dataframe """

      if service is None:
          service = get_ga_service()

      dimensions = query_data["dimensions"].keys()
      metrics = query_data["metrics"].keys()

      start = end - timedelta(c.TIMEWINDOW)

      # Query the API
      kwa = {
          "ids": "ga:{}".format(c.PROFILE),
          "start_date": "{:%Y-%m-%d}".format(start),
          "end_date": "{:%Y-%m-%d}".format(end),
          "metrics": ",".join(metrics),
          "dimensions": ",".join(dimensions),
          "max_results": c.MAX_RESULTS,
      }
      data = service.data().ga().get(**kwa).execute()

      # Create df from data obtained through the API
      columns = [x["name"] for x in data["columnHeaders"]]
      df = pd.DataFrame(data["rows"], columns=columns)

      # Handle missing values
      for x in dimensions:
          df[x] = df[x].replace("(not set)", float("nan"))

      # handle missing campaigns
      if ("ga:adwordsCampaignID" in df.columns) and ("ga:campaign" in df.columns):
          df.dropna(subset=["ga:adwordsCampaignID", "ga:campaign"], inplace=True)

      # Rename columns
      rename_dict = {i: x[0] for i, x in query_data["dimensions"].items()}
      rename_dict.update({i: x[0] for i, x in query_data["metrics"].items()})
      df = df.rename(index=str, columns=rename_dict)

      # Transform types
      for x in ["dimensions", "metrics"]:
          df = u.fix_types(df, query_data[x].values())

      log.info("Data read")

      return df
  ```

  This functions retrive a **pandas dataframe** using the requested parameters.
  After creating the raw dataframe it renames the columns using the dictionary `QUERY_DATA` from the `constants.py` file.
  It will also change the columns types using the same dictionary.

  So for exemple having:

  ```python
  QUERY_DATA = {
    "traffic_google": {
        "dimensions": {"ga:date": ("date", "date"), "ga:deviceCategory": ("device", str)},
        "metrics": {
            "ga:users": ("users", int),
            "ga:sessions": ("sessions", int),
            "ga:bounceRate": ("bounce_rate", float),
        },
    },
  }
  ```

  Will retrive the `ga:deviceCategory` dimension rename the column to `device` and store it as a string.

  Finally to run the ETL for all the tables in ``QUERY_DATA` there is the function `do_etl`:

  ```python
  def do_etl():
      """ Reads from GA, transforms the data and loads into mysql """

      time0 = time()

      for tablename, data in c.QUERY_DATA.items():

          # Retrive data from GA API
          df = get_ga_df(data)

          # Insert data into mysql
          insert_into_mysql(df, tablename)

      log.info("Data imported", time=time() - time0)
  ```

  This is what you should run every day. By default it will take the data from the last 7 days.
  This redundancy will mean that even if the ETL fail some day it can automatically retrive the missing data the next day.
  In order to lose that it must fail 7 consecutive days.

  And the last function is for retriving all old data. This is meant to only be used the first time.

  ```python
  def import_old_stats(end=date.today()):
    """ Imports old data up until 2017-01-01 """

    # Load from 2017-01-01
    while end > date(2017, 1, 1):

        time0 = time()
        start = end - timedelta(c.TIMEWINDOW)

        log.info(f"Importing data from {start:%Y-%m-%d} to {end:%Y-%m-%d}")

        for tablename, data in c.QUERY_DATA.items():

            # Retrive data from GA API
            df = get_ga_df(data, end=end)

            # Insert data into mysql
            insert_into_mysql(df, tablename)

        end -= timedelta(int(c.TIMEWINDOW) + 1)
        log.info("Data imported", time=time() - time0)
  ```

  Right now it will download all data from 2017 until today.

  ### 3.3. Load the data